## 멀티 노드 카프카 클러스터

### 멀티 브로커 구성 예시

- Kafka Broker 설정

각 broker 마다 id, port, log 경로를 지정한다.

```properties
############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=1

... 생략 ...

############################# Socket Server Settings #############################

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
listeners=PLAINTEXT://:9092

... 생략 ...

############################# Log Basics #############################

# A comma separated list of directories under which to store log files
log.dirs=/Users/hyunseo/kafka-logs-01

... 생략 ...
```

### 카프카 복제(Replication)

- Kafka 복제
  - Kafka의 복제는 데이터를 여러 Broker에 복사하여 장애 대비로 높은 가용성을 제공한다.
  - `replication-factor` 설정값으로 복제 수(원본 파티션 + 복제 파티션)를 지정할 수 있다.
  - 복제 동작은 Topic의 Partition을 대상으로 진행된다. 
  - 복제 대상의 파티션은 1개의 Leader와 N개의 Follower로 구성된다.

```bash
kafka-topics --bootstrap-server localhost:9092 --create --topic topic-p3r3 --partitions 3 --replication-factor 3
```

Kafka의 Topic 정보를 출력하면 Partition 3개와 각 Partition의 복제(leader) 수가 3인 것을 볼 수 있다. 

```bash
kafka-topics --bootstrap-server localhost:9092 --describe --topic topic-p3r3

--- 출력 결과
Topic: topic-p3r3	TopicId: kwIT-lDnSxqH3oU6gitt5g	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=1073741824,unclean.leader.election.enable=false
	Topic: topic-p3r3	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,1,3
	Topic: topic-p3r3	Partition: 1	Leader: 3	Replicas: 3,1,2	Isr: 2,1,3
	Topic: topic-p3r3	Partition: 2	Leader: 1	Replicas: 1,2,3	Isr: 2,1,3
```

![image](https://github.com/user-attachments/assets/e7150144-8be6-4ff6-8c83-c0c7f575561e)

### 복제와 리더/팔로워(leader/follower)

- Leader 역할
  - Producer, Consumer는 Leader를 통해서 read/write을 수행한다.
  - Partition 팔로우를 관리하는 Broker의 Replication도 관리한다. (Follower Fetch에 대한 응답, ISR)
- Follower 역할
  - Leader에게 주기적으로 데이터를 요청(Fetch)하여 복제

![image](https://github.com/user-attachments/assets/4890e39e-492d-4a82-ade0-02349425c3c8)

### 멀티 브로커 환경에서 Producer의 bootstrap.servers 설정

클라이언트가 Kafka에 접속하기 위해서는 일부 Broker만 알아도 된다.

이때 필요한 설정이 `bootstrap.servers`이다.

- bootstrap.servers
  - 특정한 Kafka Broker에 연결하는 설정이 아니다.
  - 하나의 Broker의 접속 정보만 설정하면 해당 Kafka Broker를 통해 Topic Partition의 Leader와 Follower들의 메타 정보를 가져와서 접속하려는 Topic의 Partition이 있는 Broker에 접속한다. 
  - 다만 하나의 Broker만 지정하면 해당 Broker 장애 시에 연결이 불가능할 수도 있으니 주의하자.

```java
public class ExampleProducer {

    public static void main(String[] args) {
        String topicName = "example-topic";

        Properties props = new Properties();
        // 9092 포트 Broker가 죽어있다면 9093 포트 Broker에 메타정보를 요청한다.
        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092, localhost:9093, localhost:9094");
        props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
    }
}
```

### 주키퍼와 컨트롤러 브로커

Kafka는 클러스터 상태 관리, leader 선출을 위해 zookeeper를 사용한다. (kafka 3.0 이후 KRaft)

zookeeper는 분산 시스템 메타 데이터를 저장/관리하고 저장의 기본 단위를 `ZNode`라 한다.

- ZNode
  - 데이터 상태 정보를 저장하는 노드
  - 디렉터리 또는 파일과 유사한 구조

```text
/
├── zookeeper
│   ├── quota
│   ├── config
│   ├── ...
│
├── brokers
│   ├── ids          # 등록된 브로커 목록
│   ├── topics       # 생성된 토픽 정보
│
├── controller       # 현재 컨트롤러 브로커 정보
├── admin
└── isr_change_notification
/
├── zookeeper
│   ├── quota
│   ├── config
│   ├── ...
│
├── brokers
│   ├── ids          # 등록된 브로커 목록
│   ├── topics       # 생성된 토픽 정보
│
├── controller       # 현재 컨트롤러 브로커 정보
├── admin
└── isr_change_notification
```

![image](https://github.com/user-attachments/assets/e2fd9755-00a4-4236-a3ad-b10401b39a12)

- zookeeper의 kafka cluster 정보 관리
  - session heartbeat
    - Kafka Broker는 주기적으로 zookeeper에 접속하여 session heartbeat를 전송하여 상태 보고
  - Broker ZNode 삭제
    - zookeeper가 `zookeeper.session.timeout.ms` 이내 HeartBeat를 받지 못하면 Broker의 ZNode 삭제
  - Controller ZNode에 변경 통보 
    - Broker ZNode 삭제 시에 Controller ZNode에 변경 사실 통보
  - 리더 선출 수행
    - Controller ZNode는 down된 Broker의 Partition들을 새로운 Partition Leader Election 수행
    - down된 Broker가 Controller일 경우 모든 노드에 통보 후 가장 먼저 접속한 다른 Broker가 Controller가 된다.

```bash
// broker#1 up, broker#2 down, broker#3 up 

get /controller
{"version":1,"brokerid":1,"timestamp":"1739619561022"}


ls /brokers/ids
[1, 3]


get /brokers/topics/topic-p3r3/partitions/0/state
{"controller_epoch":11,"leader":3,"version":1,"leader_epoch":8,"isr":[1,3]}


get /brokers/topics/topic-p3r3/partitions/1/state
{"controller_epoch":11,"leader":3,"version":1,"leader_epoch":9,"isr":[1,3]}


get /brokers/topics/topic-p3r3/partitions/2/state                    
{"controller_epoch":11,"leader":1,"version":1,"leader_epoch":8,"isr":[1,3]}


get /brokers/topics/topic-p3r3 
{"partitions":{"0":[2,3,1],"1":[3,1,2],"2":[1,2,3]},"topic_id":"kwIT-lDnSxqH3oU6gitt5g","adding_replicas":{},"removing_replicas":{},"version":3}
```

![image](https://github.com/user-attachments/assets/1e8a0290-405d-4489-acd5-c95be210a21e)

- 컨트롤러의 리더 선출 프로세스
  1. Broker#3 장애(down) 발생
  2. zookeeper 일정 시간 내에 heartbeat 못받음
  3. Controller는 Broker#3 장애 정보를 받고 Broker#3이 관리하던 Partition들에 대한 새로운 leader/follower 결정
  4. 새롭게 결정한 leader/follower 정보를 zookeeper에 저장
  5. Partition을 복제하는 모든 Broker에 새로운 leader/follower 정보 전달하고 새로운 leader로 부터 복제 수행하도록 요청
  6. Controller는 모든 Broker의 메타 정보를 새롭게 갱신하도록 요청

### ISR (In-Sync-Replicas)

- ISR
  - Kafka에서 Leader 데이터를 정상적으로 따라가고 있는 Follower의 목록
  - ISR에 포함된 Follower는 Leader와 데이터가 동기화 된 상태로 Leader 장애 시 새로운 Leader 후보가 될 수 있음
  - ISR을 유지하는 이유는 장애 발생 시 빠르게 새로운 Leader를 선출하기 위함이다.

```bash
get /brokers/topics/topic-p3r3/partitions/0/state
{"controller_epoch":11,"leader":3,"version":1,"leader_epoch":8,"isr":[1,3]}


get /brokers/topics/topic-p3r3/partitions/1/state
{"controller_epoch":11,"leader":3,"version":1,"leader_epoch":9,"isr":[1,3]}


get /brokers/topics/topic-p3r3/partitions/2/state                    
{"controller_epoch":11,"leader":1,"version":1,"leader_epoch":8,"isr":[1,3]}
```

- ISR 관리 조건
  - Broker가 zookeeper에 연결되어 있어야 한다.
  - Broker는 `zookeeper.session.timeout.ms` 내에 heartbeat를 zookeeper에 보내야 한다.
  - Follower는 `replica.lag.time.max.ms` 최대 10초(default) 내에 Leader의 메시지를 지속적으로 가져가야 한다.

![image](https://github.com/user-attachments/assets/6e135693-1bcc-4130-97dd-6c36978920ac)

- Follower의 Fetch
  - Follower는 Leader에게 Fetch(offset 번호 포함)를 요청한다.
  - Leader는 Follower의 offset 번호와 최신 offset 번호를 비교(lag)하며 데이터 복제 수행을 판단한다.
  
![image](https://github.com/user-attachments/assets/cf0aa642-163b-4636-9d99-1cef0d853f9a)

- `min.insync.replicas`
  - Producer가 `acks=all` 일 때 broker의 메시지 수신 성공에 대한 최소한의 복제 성공 개수 (잘 받을 수 있는 ISR 수)

![image](https://github.com/user-attachments/assets/3ad9d02b-9f79-4bbb-b7ad-8dc822a129d7)

### Preferred Leader Election

Kafka에서 Leader-Follower 모델을 사용하여 데이터를 복제하고, 특정 Broker 장애 시에 새로운 Leader를 선출하는 방식으로 장애를 복구한다.

- Preferred Leader Election
  - Partition의 Leader를 원래 지정된 Broker로 되돌린다.
  - Kafka는 Partition마다 Leader가 될 순서를 미리 설정해둔다.
  - Kafka는 down된 Broker가 재기동 될 때 일정 시간 이후 재선출한다.
- 설정값
  - `auto.leader.rebalance.enable=true`
  - `leader.imbalance.check.interval.seconds`: 대기할 일정 시간 (default: 300s)

![image](https://github.com/user-attachments/assets/3753a6a5-2360-491b-a772-991813d0c770)

### Unclean Leader Election

Broker#2, Broker#3이 down되고 Broker#1까지 down될 경우 문제가 생길 수 있다.

모든 Broker가 down되고 Broker#2가 up될 경우에 마지막까지 살아있던 Broker#1과의 offset이 다를 수 있다.

이때 데이터가 유실되더라도 Follower Broker를 Leader Broker로 up할지 안할지 설정할 수 있다.  

- Unclean Leader Election
  - ISR에 속하지 않은 Follower도 Leader가 될 수 있도록 허용하는 방식
  - 데이터 유실 가능성이 있지만, 가용성에 초점을 둔다.
- 설정값
  - `unclean.leader.election.enable=false`

![image](https://github.com/user-attachments/assets/4d5e1f4b-b6d5-4fe9-8371-43626a4429e7)

|  | **Preferred Leader Election** | **Unclean Leader Election** |
|--------------------------------|----------------------|----------------------|
| **목적** | 원래 Preferred Leader로 복구 | 가용성 유지 (ISR 없이도 Leader 선출) |
| **언제 실행?** | 장애 복구 후 수동 실행 | ISR이 모두 다운된 경우 |
| **데이터 유실 여부** | 데이터 유실 없음 | 데이터 유실 가능성 있음 |
| **자동 or 수동?** | 기본적으로 수동 | 설정에 따라 자동 |
| **설정 옵션** | `kafka-preferred-replica-election.sh` 실행 | `unclean.leader.election.enable=true` |
